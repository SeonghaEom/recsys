{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tr_sessions = pd.read_csv('../dataset/train_sessions_sorted.csv')\n",
    "item_feat = pd.read_csv('../dataset/item_features.csv')\n",
    "cand_items = pd.read_csv('../dataset/candidate_items.csv')\n",
    "te_l = pd.read_csv('../dataset/test_leaderboard_sessions.csv')\n",
    "te_f = pd.read_csv('../dataset/test_final_sessions.csv')\n",
    "id2idx = pd.read_csv('./id2idx.csv')\n",
    "tr_purchases = pd.read_csv('../dataset/train_purchases.csv')\n",
    "\n",
    "tr_sessions = tr_sessions.merge(id2idx, how='inner', on='item_id')\n",
    "tr_purchases = tr_purchases.merge(id2idx, how='inner', on='item_id')\n",
    "te_l = te_l.merge(id2idx, how='inner', on='item_id')\n",
    "te_f = te_f.merge(id2idx, how='inner', on='item_id')\n",
    "cand_items = cand_items.merge(id2idx, how='inner', on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23691\n",
      "4990\n",
      "18907\n",
      "4877\n",
      "14030\n",
      "113\n"
     ]
    }
   ],
   "source": [
    "print(item_feat['item_id'].nunique())\n",
    "print(cand_items['item_id'].nunique())\n",
    "print(tr_purchases.item_id.nunique())\n",
    "\n",
    "x = set(tr_purchases.item_id.unique())\n",
    "y = set(cand_items['item_id'].unique())\n",
    "z = x.intersection(y)\n",
    "print(len(z))\n",
    "print(len(x-y))\n",
    "print(len(y-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['session_id', 'Unnamed: 1', 'session_id.1', 'item_id', 'date'], dtype='object')\n",
      "73 1 73\n",
      "890 1 905\n",
      "64970\n"
     ]
    }
   ],
   "source": [
    "print(tr_sessions.columns)\n",
    "tr_sessions = tr_sessions[['session_id', 'item_id', 'date']]\n",
    "cat = item_feat['feature_category_id'].unique()\n",
    "val = item_feat['feature_value_id'].unique()\n",
    "print(len(cat),  min(cat), max(cat))\n",
    "print(len(val), min(val), max(val))\n",
    "print(len(cat) * len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### item id to list of feats to list of feat_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 67, 47, 68, 69, 22, 70, 71, 55, 72, 4, 1, 73, 74, 75]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[feat2ind[feat] for feat in item2feat[10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   session_id  item_id                     date  item_idx_x  item_idx_y\n",
      "0          61    27088  2021-06-01 08:12:39.664       22808       22808\n",
      "1       64531    27088  2021-06-06 10:15:48.917       22808       22808\n",
      "2      153529    27088   2021-06-03 09:19:30.91       22808       22808\n",
      "3      153529    27088  2021-06-03 19:20:37.292       22808       22808\n",
      "4      160053    27088  2021-06-25 09:30:36.342       22808       22808\n",
      "5      199168    27088  2021-06-15 05:25:48.506       22808       22808\n",
      "6      199168    27088  2021-06-15 05:26:18.928       22808       22808\n",
      "7      217574    27088  2021-06-03 11:54:00.129       22808       22808\n",
      "8      286801    27088  2021-06-19 21:18:44.012       22808       22808\n",
      "9      327282    27088  2021-06-25 16:56:29.304       22808       22808\n"
     ]
    }
   ],
   "source": [
    "te_l = te_l.merge(id2idx, how='inner', on='item_id')\n",
    "te_f = te_f.merge(id2idx, how='inner', on='item_id')\n",
    "print(te_f.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000005vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m./train_feat.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000005vscode-remote?line=5'>6</a>\u001b[0m   \u001b[39mfor\u001b[39;00m sid \u001b[39min\u001b[39;00m sids:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000005vscode-remote?line=6'>7</a>\u001b[0m     sess \u001b[39m=\u001b[39m tr_sessions\u001b[39m.\u001b[39;49mloc[tr_sessions[\u001b[39m'\u001b[39;49m\u001b[39msession_id\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39;49m sid][\u001b[39m'\u001b[39m\u001b[39mitem_idx\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000005vscode-remote?line=7'>8</a>\u001b[0m     sess\u001b[39m.\u001b[39mappend(tr_purchases\u001b[39m.\u001b[39mloc[tr_purchases[\u001b[39m'\u001b[39m\u001b[39msession_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m sid][\u001b[39m'\u001b[39m\u001b[39mitem_idx\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000005vscode-remote?line=8'>9</a>\u001b[0m     string \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mmap\u001b[39m(\u001b[39mstr\u001b[39m,sess)) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexing.py:931\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexing.py?line=927'>928</a>\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexing.py?line=929'>930</a>\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[0;32m--> <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexing.py?line=930'>931</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m~/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexing.py:1144\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexing.py?line=1141'>1142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_slice_axis(key, axis\u001b[39m=\u001b[39maxis)\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexing.py?line=1142'>1143</a>\u001b[0m \u001b[39melif\u001b[39;00m com\u001b[39m.\u001b[39mis_bool_indexer(key):\n\u001b[0;32m-> <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexing.py?line=1143'>1144</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getbool_axis(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexing.py?line=1144'>1145</a>\u001b[0m \u001b[39melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexing.py?line=1145'>1146</a>\u001b[0m \n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexing.py?line=1146'>1147</a>\u001b[0m     \u001b[39m# an iterable multi-selection\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexing.py?line=1147'>1148</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(labels, MultiIndex)):\n",
      "File \u001b[0;32m~/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexing.py:949\u001b[0m, in \u001b[0;36m_LocationIndexer._getbool_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexing.py?line=946'>947</a>\u001b[0m labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis)\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexing.py?line=947'>948</a>\u001b[0m key \u001b[39m=\u001b[39m check_bool_indexer(labels, key)\n\u001b[0;32m--> <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexing.py?line=948'>949</a>\u001b[0m inds \u001b[39m=\u001b[39m key\u001b[39m.\u001b[39;49mnonzero()[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexing.py?line=949'>950</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_take_with_is_copy(inds, axis\u001b[39m=\u001b[39maxis)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Generate Train dataset txt file\n",
    "\n",
    "sids = tr_purchases['session_id'].unique()\n",
    "\n",
    "with open('./train_feat.txt', 'w') as f:\n",
    "  for sid in sids:\n",
    "    sess = tr_sessions.loc[tr_sessions['session_id'] == sid]['item_idx'].tolist()\n",
    "    sess.append(tr_purchases.loc[tr_purchases['session_id'] == sid]['item_idx'].values[0])\n",
    "    string = \",\".join(map(str,sess)) + '\\n'\n",
    "    f.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23691\n",
      "item_id\n",
      "2    [56, 62, 68, 33, 72, 29, 16, 50, 61, 53, 7, 69...\n",
      "3    [56, 69, 68, 17, 32, 11, 45, 7, 19, 46, 61, 73...\n",
      "4    [55, 17, 5, 69, 18, 68, 50, 73, 65, 7, 59, 47,...\n",
      "7    [56, 50, 63, 41, 26, 32, 61, 68, 69, 7, 15, 44...\n",
      "8    [56, 55, 7, 69, 72, 3, 45, 18, 59, 73, 5, 61, ...\n",
      "Name: feature_category_id, dtype: object\n",
      "[[0 0 0 ... 0 1 0]\n",
      " [0 0 1 ... 0 1 1]\n",
      " [0 0 1 ... 0 1 1]\n",
      " ...\n",
      " [1 0 1 ... 0 1 1]\n",
      " [0 0 0 ... 0 1 1]\n",
      " [0 0 0 ... 0 1 1]]\n",
      "(23691, 73)\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73]\n",
      "item_id\n",
      "2    [365, 801, 351, 802, 75, 123, 38, 76, 462, 6, ...\n",
      "3    [365, 592, 14, 378, 902, 859, 559, 452, 254, 8...\n",
      "4    [267, 378, 605, 538, 289, 373, 317, 544, 521, ...\n",
      "7    [153, 708, 816, 759, 268, 902, 706, 739, 592, ...\n",
      "8    [365, 267, 798, 592, 75, 793, 559, 817, 856, 5...\n",
      "Name: feature_value_id, dtype: object\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "(23691, 890)\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 576, 577, 578, 579, 580, 581, 582, 583, 584, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "## check item id size\n",
    "print(item_feat['item_id'].nunique())\n",
    "# Create MultiLabelBinarizer object\n",
    "mlb = MultiLabelBinarizer()\n",
    "# One-hot encode data\n",
    "##categorical\n",
    "cat = item_feat.groupby(\"item_id\")['feature_category_id'].apply(list)\n",
    "print(cat.head())\n",
    "print(mlb.fit_transform(cat))\n",
    "multihot_cat = mlb.fit_transform(cat)\n",
    "print(multihot_cat.shape)\n",
    "print(list(mlb.classes_))\n",
    "\n",
    "##value\n",
    "val = item_feat.groupby(\"item_id\")['feature_value_id'].apply(list)\n",
    "print(val.head())\n",
    "print(mlb.fit_transform(val))\n",
    "multihot_val = mlb.fit_transform(val)\n",
    "print(multihot_val.shape)\n",
    "print(list(mlb.classes_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'multihot_cat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000007vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000007vscode-remote?line=1'>2</a>\u001b[0m concat \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhstack((multihot_cat, multihot_val))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000007vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(concat\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000007vscode-remote?line=4'>5</a>\u001b[0m np\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mconcat.npy\u001b[39m\u001b[39m\"\u001b[39m, concat)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'multihot_cat' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "concat = np.hstack((multihot_cat, multihot_val))\n",
    "\n",
    "print(concat.shape)\n",
    "np.save(\"concat.npy\", concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [1 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]] (23691, 963)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "concat = np.load(\"../processed/concat.npy\")\n",
    "print(concat[10:,], concat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0]\n",
      " [10]\n",
      " [20]\n",
      " ...\n",
      " [10]\n",
      " [ 0]\n",
      " [10]]\n",
      "(23691, 964)\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([0]*23691)\n",
    "for k,v in purid2idx.items():\n",
    "  # print(k,v)\n",
    "  arr[v] += 10\n",
    "for k,v in cand_idx.items():\n",
    "  arr[v] += 10\n",
    "arr = arr[np.newaxis]\n",
    "arr = arr.transpose()\n",
    "print(arr)\n",
    "\n",
    "concat = np.hstack((concat, arr))\n",
    "print(concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:13<00:00, 74971.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.03669725, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.04793609, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.03003003, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.09090909]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_items = concat.shape[0]\n",
    "A = np.zeros((num_items, num_items))\n",
    "print(A)\n",
    "from tqdm import tqdm\n",
    "with open('./train.txt', 'r') as f: \n",
    "  for line in tqdm(f.readlines()): # idx\n",
    "    # print(line)\n",
    "    items = list(map(int, line.split(\",\")))\n",
    "    # print(len(items))\n",
    "    for i in range(len(items)-1):\n",
    "      # print(i)\n",
    "      A[items[i]][items[i+1]] += 1\n",
    "      A[items[i+1]][items[i]] += 1\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "normed_matrix = normalize(A, axis=1, norm='l1')\n",
    "# [[ 0.          0.33333333  0.66666667]\n",
    "#  [ 0.25        0.33333333  0.41666667]\n",
    "#  [ 0.28571429  0.33333333  0.38095238]]\n",
    "\n",
    "normed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"norm_A\", normed_matrix)\n",
    "np.save(\"A\", A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08035714285714286\n",
      "18.0\n"
     ]
    }
   ],
   "source": [
    "print(normed_matrix[8122][8122])\n",
    "print(A[8122][8122])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ItemID to idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23691\n"
     ]
    }
   ],
   "source": [
    "id2idx = {}\n",
    "for i, item_id in enumerate(item_feat.item_id.unique()):\n",
    "  id2idx[item_id] = i\n",
    "print(len(id2idx.keys()))\n",
    "id2idx_df = pd.DataFrame(list(id2idx.items()),columns = ['item_id','item_idx'])\n",
    "id2idx_df.to_csv('./id2idx.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## purchased item id to idx\n",
    "tr_purchases = pd.read_csv('../dataset/train_purchases.csv')\n",
    "purid2idx = {}\n",
    "for i, iid in enumerate(tr_purchases.item_id.unique()):\n",
    "  purid2idx[iid] = id2idx[iid]\n",
    "purid2idx_df = pd.DataFrame(list(purid2idx.items()),columns = ['item_id','pur_idx'])\n",
    "purid2idx_df.to_csv('./purid2idx.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>item_idx</th>\n",
       "      <th>pur_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>13214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>18530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>9694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>15648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18902</th>\n",
       "      <td>28138</td>\n",
       "      <td>23685</td>\n",
       "      <td>10832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18903</th>\n",
       "      <td>28139</td>\n",
       "      <td>23686</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18904</th>\n",
       "      <td>28140</td>\n",
       "      <td>23687</td>\n",
       "      <td>6491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18905</th>\n",
       "      <td>28141</td>\n",
       "      <td>23688</td>\n",
       "      <td>6688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18906</th>\n",
       "      <td>28143</td>\n",
       "      <td>23690</td>\n",
       "      <td>15444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18907 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       item_id  item_idx  pur_idx\n",
       "0            3         1    10900\n",
       "1            4         2    13214\n",
       "2            7         3    18530\n",
       "3            8         4     9694\n",
       "4            9         5    15648\n",
       "...        ...       ...      ...\n",
       "18902    28138     23685    10832\n",
       "18903    28139     23686       84\n",
       "18904    28140     23687     6491\n",
       "18905    28141     23688     6688\n",
       "18906    28143     23690    15444\n",
       "\n",
       "[18907 rows x 3 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.merge(left=id2idx_df, right=purid2idx, how='inner', on='item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## candidate item id to idx\n",
    "cand = pd.read_csv('../dataset/candidate_items.csv')\n",
    "\n",
    "cand_idx = {}\n",
    "for i, iid in enumerate(cand.item_id.unique()):\n",
    "  cand_idx[iid] = id2idx[iid]\n",
    "candid2idx_df = pd.DataFrame(list(cand_idx.items()),columns = ['item_id','idx'])\n",
    "candid2idx_df.to_csv('./candid2idx_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18907\n"
     ]
    }
   ],
   "source": [
    "print(tr_purchases.item_id.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item idx to purchase &candidate idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## purchased item id to idx\n",
    "tr_purchases = pd.read_csv('../dataset/train_purchases.csv')\n",
    "## candidate item id to idx\n",
    "cand = pd.read_csv('../dataset/candidate_items.csv')\n",
    "candpurid2idx = {}\n",
    "i=0\n",
    "for i, iid in enumerate(tr_purchases.item_id.unique()):\n",
    "  candpurid2idx[iid] = i\n",
    "  i+=1\n",
    "for i, iid in enumerate(cand.item_id.unique()):\n",
    "  candpurid2idx[iid] = i\n",
    "  i+=1\n",
    "candpurid2idx_df = pd.DataFrame(list(candpurid2idx.items()),columns = ['item_id','pur_idx'])\n",
    "candpurid2idx_df.to_csv('./purcand2idx_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19020, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candpurid2idx_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SID to bought item id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   session_id  item_id                     date\n",
      "0           3    15085  2020-12-18 21:26:47.986\n",
      "1          13    18626  2020-03-13 19:36:15.507\n",
      "2          18    24911  2020-08-26 19:20:32.049\n",
      "3          19    12534   2020-11-02 17:16:45.92\n",
      "4          24    13226  2020-02-26 18:27:44.114\n"
     ]
    }
   ],
   "source": [
    "sid2iid = {}\n",
    "tr_purchases = pd.read_csv('../dataset/train_purchases.csv')\n",
    "print(tr_purchases.head())\n",
    "for i, row in tr_purchases.iterrows():\n",
    "  sid2iid[row['session_id']] = row['item_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert current trpur_sessions['item_id'] -> multilabel features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'session_id', 'item_id', 'date'], dtype='object')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "9655",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexes/base.py:3361\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3359'>3360</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3360'>3361</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3361'>3362</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/_libs/index.pyx:76\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/_libs/index.pyx:108\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 9655",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb Cell 27'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000026vscode-remote?line=0'>1</a>\u001b[0m trpur_sessions \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../dataset/trpur_sessions.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000026vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(trpur_sessions\u001b[39m.\u001b[39mcolumns)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000026vscode-remote?line=2'>3</a>\u001b[0m trpur_sessions[\u001b[39m'\u001b[39m\u001b[39mfeat\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m trpur_sessions\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m row: concat[id2idx[row[\u001b[39m'\u001b[39;49m\u001b[39mitem_id\u001b[39;49m\u001b[39m'\u001b[39;49m]]], axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000026vscode-remote?line=3'>4</a>\u001b[0m trpur_sessions[\u001b[39m'\u001b[39m\u001b[39mpur\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m trpur_sessions\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: id2idx[sid2iid[row[\u001b[39m'\u001b[39m\u001b[39msession_id\u001b[39m\u001b[39m'\u001b[39m]]], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000026vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(trpur_sessions\u001b[39m.\u001b[39mhead())\n",
      "File \u001b[0;32m~/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/frame.py:8740\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/frame.py?line=8728'>8729</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/frame.py?line=8730'>8731</a>\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/frame.py?line=8731'>8732</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/frame.py?line=8732'>8733</a>\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/frame.py?line=8737'>8738</a>\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/frame.py?line=8738'>8739</a>\u001b[0m )\n\u001b[0;32m-> <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/frame.py?line=8739'>8740</a>\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/apply.py:688\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/apply.py?line=684'>685</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/apply.py?line=685'>686</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/apply.py?line=687'>688</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/apply.py:812\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/apply.py?line=810'>811</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/apply.py?line=811'>812</a>\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/apply.py?line=813'>814</a>\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/apply.py?line=814'>815</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/apply.py:828\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/apply.py?line=824'>825</a>\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/apply.py?line=825'>826</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/apply.py?line=826'>827</a>\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/apply.py?line=827'>828</a>\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/apply.py?line=828'>829</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/apply.py?line=829'>830</a>\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/apply.py?line=830'>831</a>\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/apply.py?line=831'>832</a>\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb Cell 27'\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000026vscode-remote?line=0'>1</a>\u001b[0m trpur_sessions \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../dataset/trpur_sessions.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000026vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(trpur_sessions\u001b[39m.\u001b[39mcolumns)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000026vscode-remote?line=2'>3</a>\u001b[0m trpur_sessions[\u001b[39m'\u001b[39m\u001b[39mfeat\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m trpur_sessions\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: concat[id2idx[row[\u001b[39m'\u001b[39;49m\u001b[39mitem_id\u001b[39;49m\u001b[39m'\u001b[39;49m]]], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000026vscode-remote?line=3'>4</a>\u001b[0m trpur_sessions[\u001b[39m'\u001b[39m\u001b[39mpur\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m trpur_sessions\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: id2idx[sid2iid[row[\u001b[39m'\u001b[39m\u001b[39msession_id\u001b[39m\u001b[39m'\u001b[39m]]], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000026vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(trpur_sessions\u001b[39m.\u001b[39mhead())\n",
      "File \u001b[0;32m~/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/frame.py:3458\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/frame.py?line=3455'>3456</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/frame.py?line=3456'>3457</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/frame.py?line=3457'>3458</a>\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/frame.py?line=3458'>3459</a>\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/frame.py?line=3459'>3460</a>\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexes/base.py:3363\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3360'>3361</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3361'>3362</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3362'>3363</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3364'>3365</a>\u001b[0m \u001b[39mif\u001b[39;00m is_scalar(key) \u001b[39mand\u001b[39;00m isna(key) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhasnans:\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3365'>3366</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 9655"
     ]
    }
   ],
   "source": [
    "\n",
    "trpur_sessions = pd.read_csv('../dataset/trpur_sessions.csv')\n",
    "print(trpur_sessions.columns)\n",
    "trpur_sessions['feat'] = trpur_sessions.apply(lambda row: concat[id2idx[row['item_id']]], axis=1)\n",
    "trpur_sessions['pur'] = trpur_sessions.apply(lambda row: id2idx[sid2iid[row['session_id']]], axis=1)\n",
    "print(trpur_sessions.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23691\n"
     ]
    }
   ],
   "source": [
    "## Decompsition 9xx -> ??\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA, KernelPCA, TruncatedSVD\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "print(item_feat.item_id.nunique())\n",
    "\n",
    "def dimensionality_reduction(feat, y, kind='lda'):\n",
    "  if kind=='lda':\n",
    "    model = LinearDiscriminantAnalysis()\n",
    "  elif kind=='pca':\n",
    "    model = PCA()\n",
    "  elif kind=='kernelpca':\n",
    "    model = KernelPCA()\n",
    "  elif kind=='truncatedsvd':\n",
    "    model = TruncatedSVD()\n",
    "  elif kind==\"FA\":\n",
    "    n_components=10\n",
    "    model = FactorAnalysis(n_components=n_components, random_state=0)\n",
    "  \n",
    "    new_feat = model.fit_transform(feat, y)\n",
    "    print(\"new feat shape\", new_feat.shape)\n",
    "    np.save(\"../processed/{}_{}_feat.npy\".format(str(kind),n_components), new_feat)\n",
    "  return new_feat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new feat shape (23691, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.01354743e+00, -2.36522428e-01, -1.78191219e-01, ...,\n",
       "        -1.92526866e+00,  4.54186979e-02,  1.46163958e-01],\n",
       "       [-9.04081155e-01, -2.62624712e-01,  1.90069086e-02, ...,\n",
       "         3.56022303e-04, -1.32319546e+00,  3.95381329e-02],\n",
       "       [-9.04081155e-01, -2.62624712e-01,  1.90069086e-02, ...,\n",
       "         3.33666356e-04, -1.33722074e+00, -3.97352781e-01],\n",
       "       ...,\n",
       "       [-1.07802177e+00, -3.72028055e-01,  4.30769278e-02, ...,\n",
       "         5.60497418e-04, -3.23578789e-01, -7.00453048e-01],\n",
       "       [-9.04081155e-01, -2.62624712e-01,  1.90069086e-02, ...,\n",
       "        -1.46177380e-03,  2.09284745e+00,  7.71320903e-01],\n",
       "       [-9.04081155e-01, -2.62624712e-01,  1.90069086e-02, ...,\n",
       "         2.25556626e-04, -3.45470278e-01,  2.23610558e+00]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trpur_feat = trpur_sessions['feat'].to_list()\n",
    "\n",
    "dimensionality_reduction(concat, None, 'FA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trpur_feat = trpur_sessions['feat'].to_list()\n",
    "y = trpur_sessions['pur'].to_list()\n",
    "lda_feat = dimensionality_reduction(trpur_feat, y, 'lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new feat shape (5743820, 963)\n"
     ]
    }
   ],
   "source": [
    "pca_feat = dimensionality_reduction(trpur_feat, y, 'pca')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 240. TiB for an array with shape (5743820, 5743820) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb Cell 29'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000035vscode-remote?line=0'>1</a>\u001b[0m kernelpca_feat \u001b[39m=\u001b[39m dimensionality_reduction(trpur_feat, y, \u001b[39m'\u001b[39;49m\u001b[39mkernelpca\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000035vscode-remote?line=1'>2</a>\u001b[0m tsvd_feat \u001b[39m=\u001b[39m dimensionality_reduction(trpur_feat, y, \u001b[39m'\u001b[39m\u001b[39mtruncatedsvd\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb Cell 26'\u001b[0m in \u001b[0;36mdimensionality_reduction\u001b[0;34m(feat, y, kind)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000026vscode-remote?line=12'>13</a>\u001b[0m \u001b[39melif\u001b[39;00m kind\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtruncatedsvd\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000026vscode-remote?line=13'>14</a>\u001b[0m   model \u001b[39m=\u001b[39m TruncatedSVD()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000026vscode-remote?line=15'>16</a>\u001b[0m new_feat \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit_transform(feat, y)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000026vscode-remote?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnew feat shape\u001b[39m\u001b[39m\"\u001b[39m, new_feat\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000026vscode-remote?line=17'>18</a>\u001b[0m np\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39m./\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.npy\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mstr\u001b[39m(kind)), new_feat)\n",
      "File \u001b[0;32m~/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py:456\u001b[0m, in \u001b[0;36mKernelPCA.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py?line=434'>435</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_transform\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams):\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py?line=435'>436</a>\u001b[0m     \u001b[39m\"\"\"Fit the model from data in X and transform X.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py?line=436'>437</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py?line=437'>438</a>\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py?line=453'>454</a>\u001b[0m \u001b[39m        Returns the instance itself.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py?line=454'>455</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py?line=455'>456</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py?line=457'>458</a>\u001b[0m     \u001b[39m# no need to use the kernel to transform X, use shortcut expression\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py?line=458'>459</a>\u001b[0m     X_transformed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meigenvectors_ \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meigenvalues_)\n",
      "File \u001b[0;32m~/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py:423\u001b[0m, in \u001b[0;36mKernelPCA.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py?line=420'>421</a>\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(X, accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy_X)\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py?line=421'>422</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_centerer \u001b[39m=\u001b[39m KernelCenterer()\n\u001b[0;32m--> <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py?line=422'>423</a>\u001b[0m K \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_kernel(X)\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py?line=423'>424</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_transform(K)\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py?line=425'>426</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_inverse_transform:\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py?line=426'>427</a>\u001b[0m     \u001b[39m# no need to use the kernel to transform X, use shortcut expression\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py:302\u001b[0m, in \u001b[0;36mKernelPCA._get_kernel\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py?line=299'>300</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py?line=300'>301</a>\u001b[0m     params \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mgamma\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma, \u001b[39m\"\u001b[39m\u001b[39mdegree\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdegree, \u001b[39m\"\u001b[39m\u001b[39mcoef0\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef0}\n\u001b[0;32m--> <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py?line=301'>302</a>\u001b[0m \u001b[39mreturn\u001b[39;00m pairwise_kernels(\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py?line=302'>303</a>\u001b[0m     X, Y, metric\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel, filter_params\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/decomposition/_kernel_pca.py?line=303'>304</a>\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/metrics/pairwise.py:2048\u001b[0m, in \u001b[0;36mpairwise_kernels\u001b[0;34m(X, Y, metric, filter_params, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=2044'>2045</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=2045'>2046</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnknown kernel \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m metric)\n\u001b[0;32m-> <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=2047'>2048</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _parallel_pairwise(X, Y, func, n_jobs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/metrics/pairwise.py:1425\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1421'>1422</a>\u001b[0m X, Y, dtype \u001b[39m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1423'>1424</a>\u001b[0m \u001b[39mif\u001b[39;00m effective_n_jobs(n_jobs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1424'>1425</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(X, Y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1426'>1427</a>\u001b[0m \u001b[39m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1427'>1428</a>\u001b[0m fd \u001b[39m=\u001b[39m delayed(_dist_wrapper)\n",
      "File \u001b[0;32m~/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/metrics/pairwise.py:1068\u001b[0m, in \u001b[0;36mlinear_kernel\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1044'>1045</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1045'>1046</a>\u001b[0m \u001b[39mCompute the linear kernel between X and Y.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1046'>1047</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1064'>1065</a>\u001b[0m \u001b[39mGram matrix : ndarray of shape (n_samples_X, n_samples_Y)\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1065'>1066</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1066'>1067</a>\u001b[0m X, Y \u001b[39m=\u001b[39m check_pairwise_arrays(X, Y)\n\u001b[0;32m-> <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/metrics/pairwise.py?line=1067'>1068</a>\u001b[0m \u001b[39mreturn\u001b[39;00m safe_sparse_dot(X, Y\u001b[39m.\u001b[39;49mT, dense_output\u001b[39m=\u001b[39;49mdense_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/utils/extmath.py:153\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/utils/extmath.py?line=150'>151</a>\u001b[0m         ret \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(a, b)\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/utils/extmath.py?line=151'>152</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/utils/extmath.py?line=152'>153</a>\u001b[0m     ret \u001b[39m=\u001b[39m a \u001b[39m@\u001b[39;49m b\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/utils/extmath.py?line=154'>155</a>\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/utils/extmath.py?line=155'>156</a>\u001b[0m     sparse\u001b[39m.\u001b[39missparse(a)\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/utils/extmath.py?line=156'>157</a>\u001b[0m     \u001b[39mand\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(b)\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/utils/extmath.py?line=157'>158</a>\u001b[0m     \u001b[39mand\u001b[39;00m dense_output\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/utils/extmath.py?line=158'>159</a>\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(ret, \u001b[39m\"\u001b[39m\u001b[39mtoarray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/utils/extmath.py?line=159'>160</a>\u001b[0m ):\n\u001b[1;32m    <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/sklearn/utils/extmath.py?line=160'>161</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39mtoarray()\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 240. TiB for an array with shape (5743820, 5743820) and data type float64"
     ]
    }
   ],
   "source": [
    "kernelpca_feat = dimensionality_reduction(trpur_feat, y, 'kernelpca')\n",
    "tsvd_feat = dimensionality_reduction(trpur_feat, y, 'truncatedsvd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## play around embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "weight = torch.FloatTensor(concat)\n",
    "embedding = nn.Embedding.from_pretrained(weight, freeze=False, max_norm=1, norm_type=2.0)\n",
    "input = torch.LongTensor([1])\n",
    "print(embedding(input))\n",
    "print(embedding(input).norm(dim=-1))\n",
    "embedding.weight\n",
    "embedding(input).mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.1443, 0.1443, 0.1443, 0.0000, 0.1443, 0.0000, 0.0000,\n",
      "         0.0000, 0.1443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1443, 0.1443,\n",
      "         0.1443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1443, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1443, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1443,\n",
      "         0.1443, 0.1443, 0.0000, 0.0000, 0.1443, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1443, 0.1443, 0.0000, 0.0000, 0.1443, 0.0000, 0.1443, 0.0000, 0.1443,\n",
      "         0.0000, 0.1443, 0.0000, 0.0000, 0.1443, 0.1443, 0.0000, 0.0000, 0.1443,\n",
      "         0.1443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1443, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1443, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1443, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1443, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1443, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1443, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.1443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1443, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1443, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1443, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.1443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1443,\n",
      "         0.0000, 0.1443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1443, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1443, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<EmbeddingBagBackward0>)\n",
      "tensor([1.0000], grad_fn=<CopyBackwards>)\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2000, 0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.2000, 0.0000,\n",
      "         0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.2000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.2000, 0.0000, 0.0000, 0.2000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.2000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<EmbeddingBagBackward0>)\n",
      "tensor([[0.0000, 0.0000, 0.0722, 0.0722, 0.0722, 0.0000, 0.1722, 0.0000, 0.0000,\n",
      "         0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0722, 0.0722,\n",
      "         0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0722, 0.0000,\n",
      "         0.0000, 0.1000, 0.0000, 0.0000, 0.0722, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0722,\n",
      "         0.0722, 0.1722, 0.0000, 0.0000, 0.1722, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0722, 0.1722, 0.0000, 0.0000, 0.0722, 0.0000, 0.1722, 0.1000, 0.0722,\n",
      "         0.0000, 0.0722, 0.0000, 0.0000, 0.1722, 0.1722, 0.0000, 0.0000, 0.1722,\n",
      "         0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1722, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.1000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0722,\n",
      "         0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<EmbeddingBagBackward0>)\n"
     ]
    }
   ],
   "source": [
    "emb_bag = nn.EmbeddingBag.from_pretrained(weight, freeze=False, max_norm=1, norm_type=2.0, )\n",
    "input = torch.LongTensor([1])\n",
    "offset = torch.LongTensor([0])\n",
    "print(emb_bag(input, offset))\n",
    "print(emb_bag(input, offset).norm(dim=-1))\n",
    "indice = torch.LongTensor([0])\n",
    "print(emb_bag(indice, offset))\n",
    "indices = torch.LongTensor([0,1])\n",
    "print(emb_bag(indices, offset))\n",
    "emb_bag.weight\n",
    "emb_bag(input, offset).mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000, 0.0722, 0.0722, 0.0722, 0.0000, 0.1722, 0.0000, 0.0000,\n",
      "         0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0722, 0.0722,\n",
      "         0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0722, 0.0000,\n",
      "         0.0000, 0.1000, 0.0000, 0.0000, 0.0722, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0722,\n",
      "         0.0722, 0.1722, 0.0000, 0.0000, 0.1722, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0722, 0.1722, 0.0000, 0.0000, 0.0722, 0.0000, 0.1722, 0.1000, 0.0722,\n",
      "         0.0000, 0.0722, 0.0000, 0.0000, 0.1722, 0.1722, 0.0000, 0.0000, 0.1722,\n",
      "         0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1722, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.1000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0722,\n",
      "         0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0722, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0722, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<EmbeddingBagBackward0>)\n"
     ]
    }
   ],
   "source": [
    "indices=torch.LongTensor([[1,0]])\n",
    "print(emb_bag(indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "FA_128_feat = t.FloatTensor(np.load('./FA_128_feat.npy'))\n",
    "normed_matrix = t.FloatTensor(np.load('./norm_A.npy'))\n",
    "in_feat = 128\n",
    "out_feat = 19021\n",
    "device = t.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=128, out_features=19021, bias=True)\n",
      "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat):\n",
    "      super(Net, self).__init__()\n",
    "\n",
    "      self.fc1 = nn.Linear(in_feat, out_feat)\n",
    "      self.dropout1 = nn.Dropout2d(0.25)\n",
    "      # self.fc2 = nn.Linear(523, 19021)\n",
    "\n",
    "    def forward(self, x):\n",
    "      x = self.fc1(x)\n",
    "      x = t.relu(x)\n",
    "\n",
    "      # Apply softmax to x\n",
    "      output = F.log_softmax(x, dim=1)\n",
    "      return output\n",
    "\n",
    "\n",
    "my_nn = Net(in_feat, out_feat).to(device)\n",
    "print(my_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_items = 23691\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DataLoader:\n",
    "  def __init__(self, fn, bs):\n",
    "    self.fn = fn\n",
    "    self.bs = bs\n",
    "  def __iter__(self):\n",
    "    with open(self.fn, 'r') as f: \n",
    "      for line in tqdm(f.readlines()): # idx\n",
    "        # print(line)\n",
    "        train = np.zeros((num_items, num_items))\n",
    "        label = np.zeros(num_items)\n",
    "        items = list(map(int, line.split(\",\")))\n",
    "        # print(len(items))\n",
    "        for i in range(len(items)-2):\n",
    "          # print(i)\n",
    "          train[items[i]][items[i+1]] = 1\n",
    "          train[items[i+1]][items[i]] = 1\n",
    "        label[items[-1]] = 1\n",
    "        yield train, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPRLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logit):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logit (BxB): Variable that stores the logits for the items in the mini-batch\n",
    "                         The first dimension corresponds to the batches, and the second\n",
    "                         dimension corresponds to sampled number of items to evaluate\n",
    "        \"\"\"\n",
    "        # differences between the item scores\n",
    "        diff = logit.diag().view(-1, 1).expand_as(logit) - logit\n",
    "        # final loss\n",
    "        loss = -torch.mean(F.logsigmoid(diff))\n",
    "        return loss\n",
    "\n",
    "\n",
    "class BPR_max(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPR_max, self).__init__()\n",
    "    def forward(self, logit):\n",
    "        logit_softmax = F.softmax(logit, dim=1)\n",
    "        diff = logit.diag().view(-1, 1).expand_as(logit) - logit\n",
    "        loss = -torch.log(torch.mean(logit_softmax * torch.sigmoid(diff)))\n",
    "        return loss\n",
    "\n",
    "loss_func = BPR_max()\n",
    "\n",
    "optim = t.optim.Adam(my_nn.parameters(), lr = 0.01, weight_decay = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:01, ?it/s]\n",
      "  0%|          | 0/1000000 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.68 GiB (GPU 0; 9.78 GiB total capacity; 5.07 GiB already allocated; 425.56 MiB free; 5.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb Cell 42'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000042vscode-remote?line=6'>7</a>\u001b[0m label \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mFloatTensor(label)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000042vscode-remote?line=7'>8</a>\u001b[0m \u001b[39m# label = t.FloatTensor(label).to(device)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000042vscode-remote?line=9'>10</a>\u001b[0m logit \u001b[39m=\u001b[39m my_nn(sess)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000042vscode-remote?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_func(logit, label)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000042vscode-remote?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(loss)\n",
      "File \u001b[0;32m~/anaconda3/envs/serec/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/seongha/anaconda3/envs/serec/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb Cell 38'\u001b[0m in \u001b[0;36mNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000038vscode-remote?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000038vscode-remote?line=9'>10</a>\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000038vscode-remote?line=10'>11</a>\u001b[0m   x \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mrelu(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000038vscode-remote?line=12'>13</a>\u001b[0m   \u001b[39m# Apply softmax to x\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B143.248.157.143/home/seongha/GRU4REC-pytorch/notebooks/input_encoding.ipynb#ch0000038vscode-remote?line=13'>14</a>\u001b[0m   output \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mlog_softmax(x, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.68 GiB (GPU 0; 9.78 GiB total capacity; 5.07 GiB already allocated; 425.56 MiB free; 5.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainloader = DataLoader('./train.txt', 1)\n",
    "AX = t.matmul(normed_matrix, FA_128_feat)\n",
    "for i, (sess, label) in tqdm(enumerate(trainloader)):\n",
    "  # print(sess,label)\n",
    "  sess = t.FloatTensor(sess)\n",
    "  sess = t.matmul(sess, AX).to(device) ## 23691 * 128\n",
    "  label = t.FloatTensor(label).to(device)\n",
    "  # label = t.FloatTensor(label).to(device)\n",
    "  \n",
    "  logit = my_nn(sess)\n",
    "  loss = loss_func(logit, label)\n",
    "  print(loss)\n",
    "  loss.backward()\n",
    "  optim.step()\n",
    "  del sess\n",
    "  del label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6795b7e335e45d28e9efdbdc5fd8d01cb8d10405706726ed248b9c8edb4faec"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('serec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
