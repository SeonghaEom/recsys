{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataBefore = './dataset/train_sessions.csv' #Path to Original Training Dataset \"Clicks\" File\n",
    "dataBefore = './dataset/trpur_sessions.csv' #Path to Original Training Dataset \"Clicks\" File\n",
    "dataTestBefore = './dataset/test_leaderboard_sessions.csv' #Path to Original Testing Dataset \"Clicks\" File\n",
    "dataTestFinalBefore = './dataset/test_final_sessions.csv' #Path to Original Testing Dataset \"Clicks\" File\n",
    "dataAfter = './processed/' #Path to Processed Dataset Folder\n",
    "dayTime = 86400 #Validation Only one day = 86400 seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def removeShortSessions(data):\n",
    "    #delete sessions of length < 1\n",
    "    sessionLen = data.groupby('session_id').size() #group by sessionID and get size of each session\n",
    "    data = data[np.in1d(data.session_id, sessionLen[sessionLen > 1].index)]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'session_id', 'item_id', 'date'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train= pd.read_csv(dataBefore)\n",
    "print(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index(['session_id', 'item_id', 'date'], dtype='object')\n",
      "   session_id item_id                     date\n",
      "0           3    9655  2020-12-18 21:19:48.093\n",
      "1           3    9655  2020-12-18 21:25:00.373\n",
      "2           3   15085  2020-12-18 21:26:47.986\n",
      "3          13   15654  2020-03-13 19:35:27.136\n",
      "4          13   18626  2020-03-13 19:36:15.507\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(dataBefore, usecols=[1,2,3], dtype={0:np.int32, 1:np.int64, 2:str})\n",
    "print(type(train))\n",
    "print(train.columns)\n",
    "print(train.head())\n",
    "\n",
    "#Read Dataset in pandas Dataframe (Ignore Category Column)\n",
    "# train = pd.read_csv(dataBefore, sep=',', dtype={0:np.int32, 1:np.int64, 2:str})\n",
    "test = pd.read_csv(dataTestBefore, sep=',', dtype={0:np.int32, 1:np.int64, 2:str})\n",
    "test_f = pd.read_csv(dataTestFinalBefore, sep=',', dtype={0:np.int32, 1:np.int64, 2:str})\n",
    "# train.columns = ['SessionID','ItemID', 'Time', ] #Headers of dataframe\n",
    "# test.columns = ['SessionID', 'ItemID', 'Time', ] #Headers of dataframe\n",
    "train['date'] = pd.to_datetime(train['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "test['date'] = pd.to_datetime(test['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "test_f['date'] = pd.to_datetime(test_f['date'], format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   session_id  item_id          date\n",
      "0          61    27088  1.622503e+09\n",
      "1          96    11693  1.624092e+09\n",
      "2          96    18298  1.624093e+09\n",
      "3          96     4738  1.624093e+09\n",
      "4          96      495  1.624093e+09\n"
     ]
    }
   ],
   "source": [
    "## convert to timestamp\n",
    "import time\n",
    "train['date'] = train['date'].apply(lambda date: time.mktime(date.timetuple()))\n",
    "test['date'] = test['date'].apply(lambda date: time.mktime(date.timetuple()))\n",
    "test_f['date'] = test_f['date'].apply(lambda date: time.mktime(date.timetuple()))\n",
    "print(test_f.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #sort by date\n",
    "# train = train.groupby(['session_id']).apply(lambda x: x.sort_values(['date'], ascending=True))\n",
    "# print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove sessions of less than 2 interactions\n",
    "train = removeShortSessions(train)\n",
    "\n",
    "# #delete records of items which appeared less than 5 times\n",
    "# itemLen = train.groupby('item_id').size() #groupby item_id and get size of each item\n",
    "# train = train[np.in1d(train.item_id, itemLen[itemLen > 4].index)]\n",
    "\n",
    "#remove sessions of less than 2 interactions again\n",
    "train = removeShortSessions(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Delete records in testing split where items are not in training split TODO\n",
    "# test = test[np.in1d(test.item_id, train.item_id)]\n",
    "\n",
    "#Delete Sessions in testing split which are less than 2\n",
    "test = removeShortSessions(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Training Set has 5743820 Events,  1000000 Sessions, and 23618 Items\n",
      "\n",
      "\n",
      "Testing Set has 229354 Events,  50000 Sessions, and 5647 Items\n",
      "\n",
      "\n",
      "Testing Set has 226138 Events,  50000 Sessions, and 5648 Items\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Convert To CSV\n",
    "print('Full Training Set has', len(train), 'Events, ', train.session_id.nunique(), 'Sessions, and', train.item_id.nunique(), 'Items\\n\\n')\n",
    "#train.to_csv(dataAfter + 'recSys15TrainFull.txt', sep='\\t', index=False)\n",
    "print('Testing Set has', len(test), 'Events, ', test.session_id.nunique(), 'Sessions, and', test.item_id.nunique(), 'Items\\n\\n')\n",
    "test.to_csv(dataAfter + 'test.txt', sep=',', index=False)\n",
    "\n",
    "print('Testing Set has', len(test_f), 'Events, ', test_f.session_id.nunique(), 'Sessions, and', test_f.item_id.nunique(), 'Items\\n\\n')\n",
    "test_f.to_csv(dataAfter + 'test_f.txt', sep=',', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set has 4595056 Events,  799608 Sessions, and 23119 Items\n",
      "\n",
      "\n",
      "Validation Set has 1148764 Events,  200393 Sessions, and 20412 Items\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Separate Training set into Train and Validation Splits\n",
    "# timeMax = train.date.max()\n",
    "# sessionMaxTime = train.groupby('session_id').date.max()\n",
    "# sessionTrain = sessionMaxTime[sessionMaxTime < (timeMax - dayTime)].index #training split is all sessions that ended before the last 2nd day\n",
    "# sessionValid = sessionMaxTime[sessionMaxTime >= (timeMax - dayTime)].index #validation split is all sessions that ended during the last 2nd day\n",
    "# trainTR = train[np.in1d(train.SessionID, sessionTrain)]\n",
    "# trainVD = train[np.in1d(train.SessionID, sessionValid)]\n",
    "# #Delete records in validation split where items are not in training split\n",
    "# trainVD = trainVD[np.in1d(trainVD.ItemID, trainTR.ItemID)]\n",
    "# #Delete Sessions in testing split which are less than 2\n",
    "\n",
    "splitlen = int(len(train) * 0.9)\n",
    "trainTR = train.iloc[:splitlen,:] #first 0.9\n",
    "trainVD = train.iloc[splitlen:,:] #last 0.1\n",
    "\n",
    "trainVD = removeShortSessions(trainVD)\n",
    "#Convert To CSV\n",
    "print('Training Set has', len(trainTR), 'Events, ', trainTR.session_id.nunique(), 'Sessions, and', trainTR.item_id.nunique(), 'Items\\n\\n')\n",
    "trainTR.to_csv(dataAfter + 'train.txt', sep=',', index=False)\n",
    "print('Validation Set has', len(trainVD), 'Events, ', trainVD.session_id.nunique(), 'Sessions, and', trainVD.item_id.nunique(), 'Items\\n\\n')\n",
    "trainVD.to_csv(dataAfter + 'valid.txt', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. nn.Embedding for item categorica features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.5856,  0.9176, -0.7671, -0.3828,  0.4716],\n",
      "        [-1.2720,  0.0956, -2.0452,  0.1737, -0.2470],\n",
      "        [-0.8038,  1.9868, -0.6960,  1.3874,  2.3874]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "n, d, m = 3, 5, 7\n",
    "embedding = nn.Embedding(n, d, max_norm=True)\n",
    "print(embedding.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4476,  1.2224,  0.5746,  0.7182,  0.9629,  0.0524, -0.7697],\n",
      "        [-1.7727,  0.6874, -1.2663,  0.4730, -0.7384,  0.8012, -0.7183],\n",
      "        [ 0.3871, -0.0684,  2.4013,  0.0066, -0.8426,  1.2806,  0.8540],\n",
      "        [-0.9335, -0.9808,  0.3194,  0.1850,  0.8160, -0.3397,  1.5963],\n",
      "        [-0.9586, -0.3076,  0.1421,  0.3562,  0.6051, -0.1612, -0.1200]],\n",
      "       grad_fn=<TBackward0>)\n",
      "tensor([[-2.2805,  1.6296, -2.7227,  0.9467,  0.5055, -0.1623, -2.4327],\n",
      "        [-0.1306, -0.5943, -2.3640, -0.3860,  0.1730, -1.0819, -0.2179],\n",
      "        [-1.9683, -0.4670, -1.0849,  0.4111,  0.2587, -0.0554,  0.1474]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "tensor([[-0.1306, -0.5943, -2.3640, -0.3860,  0.1730, -1.0819, -0.2179],\n",
      "        [-1.9683, -0.4670, -1.0849,  0.4111,  0.2587, -0.0554,  0.1474]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "tensor(5.0869e-30, grad_fn=<ProdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "W = torch.randn((m, d), requires_grad=True)\n",
    "print(W.t())\n",
    "idx = torch.tensor([1, 2])\n",
    "a = embedding.weight.clone() @ W.t()  # weight must be cloned for this to be differentiable\n",
    "print(a)\n",
    "b = embedding(idx) @ W.t()  # modifies weight in-place\n",
    "print(b)\n",
    "out = (a.unsqueeze(0) + b.unsqueeze(1))\n",
    "loss = out.sigmoid().prod()\n",
    "print(loss)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = torch.FloatTensor([[1, 0, 0], [0, 1, 1]])\n",
    "embedding = nn.Embedding.from_pretrained(weight)\n",
    "input = torch.LongTensor([1])\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6795b7e335e45d28e9efdbdc5fd8d01cb8d10405706726ed248b9c8edb4faec"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('serec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
